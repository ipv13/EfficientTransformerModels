# -*- coding: utf-8 -*-
"""NAM_RNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cyRQeMhqqaO4jh8tqKnq680JyiaVloJS
"""

# This code installs the yfinance library, which allows you to access financial data from Yahoo Finance
!pip install yfinance

# Import the yfinance library and assign it to the alias 'yf'
import yfinance as yf

# Create a list of tickers for the stocks you want to retrieve data for
list_tickers = ['CL=F','^GSPC', '^TNX', 'XOM']

# Use the Tickers() function from yfinance to create an object for each ticker in the list
tickers = yf.Tickers(list_tickers)

# Use the download() function from yfinance to download the stock data for the specified date range
data = yf.download(list_tickers,  start="2007-06-01", end="2022-07-01")

# Extract the 'Close' column from the dataframe
data = data['Close']

# Print the dataframe
data

# Remove all rows with missing values (NaN) from the dataframe
data = data.dropna()

# Check if the dataframe still contains any missing values
# returns False if there is no missing values
data.isnull().values.any()

# Select specific columns from the dataframe
data = data[['CL=F','^GSPC', '^TNX', 'XOM']]

# Import numpy and train_test_split function from sklearn
import numpy as np
from sklearn.model_selection import train_test_split

# Split the data into a training and test set, with a 80/20 split
# test_size = 0.2 means 20% of the data will be used for testing
training_set, test_set = train_test_split(data, test_size=0.2)

# Convert the training and test sets to numpy arrays
training_set = training_set.values
test_set = test_set.values

# Import the StandardScaler class from the sklearn.preprocessing module
from sklearn.preprocessing import StandardScaler

# Create an instance of the StandardScaler class
sc = StandardScaler()

# Fit the StandardScaler to the training set and transform it
# This centers and scales the data so that the mean is 0 and the standard deviation is 1
training_set_scaled = sc.fit_transform(training_set)

# Apply the same scaling to the test set
test_set_scaled = sc.transform(test_set)

test_set_scaled.shape

# Extract all columns from the training set except for the last column and assign to X_train
X_train = training_set_scaled[:,:-1]
# Extract the last column from the training set and assign to y_train
y_train = training_set_scaled[:,-1]
X_train, y_train

# Select all rows and all columns except for the last one in the test set to be used as the independent variables
X_test = test_set_scaled[:,:-1]
# Select all rows and only the last column in the test set to be used as the dependent variable
y_test = test_set_scaled[:,-1]

import numpy as np
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

from tensorflow import keras
import numpy as np
import tensorflow as tf

num_unique_vals = [len(np.unique(X_train[:, i])) for i in range(X_train.shape[1])]
num_units = [min(1000, i * 2) for i in num_unique_vals]
num_inputs = X_train.shape[-2]

# Activation Functions
def relu(x, weight, bias):
  """ReLU activation."""
  return tf.nn.relu(weight * (x - bias))

def relu_n(x, n = 1):
  """ReLU activation clipped at n."""
  return tf.clip_by_value(x, 0, n)

def exu(x, weight, bias):
  """ExU hidden unit modification."""
  return tf.exp(weight) * (x - bias)

def create_nam_model(x_train,
                     dropout,
                     feature_dropout = 0.0,
                     num_basis_functions = 1000,
                     units_multiplier = 2,
                     activation = 'exu',
                     name_scope = 'model',
                     shallow = False,
                     trainable = True):
  """Create the NAM model."""
  num_unique_vals = [
      len(np.unique(x_train[:, i])) for i in range(x_train.shape[1])
  ]
  num_units = [
      min(num_basis_functions, i * units_multiplier) for i in num_unique_vals
  ]
  num_inputs = x_train.shape[-2]
  nn_model = NAM(
      num_inputs=num_inputs,
      num_units=num_units,
      dropout=np.float32(dropout),
      feature_dropout=np.float32(feature_dropout),
      activation=activation,
      shallow=shallow,
      trainable=trainable,
      name_scope=name_scope)
  return nn_model

class ActivationLayer(tf.keras.layers.Layer):
  """Custom activation Layer to support ExU hidden units."""

  def __init__(self,
               num_units,
               name = None,
               activation = 'exu',
               trainable = True):
    """Initializes ActivationLayer hyperparameters.

    Args:
      num_units: Number of hidden units in the layer.
      name: The name of the layer.
      activation: Activation to use. The default value of `None` corresponds to
        using the ReLU-1 activation with ExU units while `relu` would use
        standard hidden units with ReLU activation.
      trainable: Whether the layer parameters are trainable or not.
    """
    super(ActivationLayer, self).__init__(trainable=trainable, name=name)
    self.num_units = num_units
    self._trainable = trainable
    if activation == 'relu':
      self._activation = relu
      self._beta_initializer = 'glorot_uniform'
    elif activation == 'exu':
      self._activation = lambda x, weight, bias: relu_n(exu(x, weight, bias))
      self._beta_initializer = tf.initializers.truncated_normal(
          mean=4.0, stddev=0.5)
    else:
      raise ValueError('{} is not a valid activation'.format(activation))

  def build(self, input_shape):
    """Builds the layer weight and bias parameters."""
    self._beta = self.add_weight(
        name='beta',
        shape=[input_shape[-1], self.num_units],
        initializer=self._beta_initializer,
        trainable=self._trainable)
    self._c = self.add_weight(
        name='c',
        shape=[1, self.num_units],
        initializer=tf.initializers.truncated_normal(stddev=0.5),
        trainable=self._trainable)
    super(ActivationLayer, self).build(input_shape)

  @tf.function
  def call(self, x):
    """Computes the output activations."""
    center = tf.tile(self._c, [tf.shape(x)[0], 1])
    out = self._activation(x, self._beta, center)
    return out


class FeatureRNN(tf.keras.layers.Layer):

  def __init__(self,
               num_units,
               dropout = 0.5,
               trainable = True,
               shallow = False,
               feature_num = 0,
               name_scope = 'model',
               activation = 'exu'):

    super(FeatureRNN, self).__init__()
    self._num_units = num_units
    self._dropout = dropout
    self._trainable = trainable
    self._tf_name_scope = name_scope
    self._feature_num = feature_num
    self._shallow = shallow
    self._activation = activation

  def build(self, input_shape):
    """Builds the feature net layers."""
    self.hidden_layers = [
        ActivationLayer(
            self._num_units,
            trainable=self._trainable,
            activation=self._activation,
            name='activation_layer_{}'.format(self._feature_num))
    ]
    if not self._shallow:
      self._h1 = tf.keras.layers.LSTM(
          64,
          return_sequences=True,
          trainable=self._trainable,
          name='h1_{}'.format(self._feature_num))
      self._h2 = tf.keras.layers.LSTM(
          32,
          trainable=self._trainable,
          name='h2_{}'.format(self._feature_num))
      self.hidden_layers += [self._h1, self._h2]
    self.linear = tf.keras.layers.Dense(
        1,
        use_bias=False,
        trainable=self._trainable,
        name='dense_{}'.format(self._feature_num),
        kernel_initializer='glorot_uniform')
    super(FeatureRNN, self).build(input_shape)

  @tf.function
  def call(self, x, training):
    """Computes FeatureNN output with either evaluation or training mode."""
    with tf.name_scope(self._tf_name_scope):
      for l in self.hidden_layers:
        x = tf.nn.dropout(
            l(x), rate=0.05)
      x = tf.squeeze(self.linear(x), axis=1)
    return x


class NAM(tf.keras.Model):

  def __init__(self,
               num_inputs,
               num_units,
               trainable = True,
               shallow = False,
               feature_dropout = 0.0,
               dropout = 0.0,
               **kwargs):

    super(NAM, self).__init__()
    self._num_inputs = num_inputs
    if isinstance(num_units, list):
      assert len(num_units) == num_inputs
      self._num_units = num_units
    elif isinstance(num_units, int):
      self._num_units = [num_units for _ in range(self._num_inputs)]
    self._trainable = trainable
    self._shallow = shallow
    self._feature_dropout = feature_dropout
    self._dropout = dropout
    self._kwargs = kwargs

  def build(self, input_shape):
    self.feature_rnns = [None] * self._num_inputs
    for i in range(self._num_inputs):
      self.feature_rnns[i] = FeatureRNN(
          num_units=self._num_units[i],
          dropout=self._dropout,
          trainable=self._trainable,
          shallow=self._shallow,
          feature_num=i,
          **self._kwargs)
    self._bias = self.add_weight(
        name='bias',
        initializer=tf.keras.initializers.Zeros(),
        shape=(1,),
        trainable=self._trainable)
    self._true = tf.constant(True, dtype=tf.bool)
    self._false = tf.constant(False, dtype=tf.bool)

  def call(self, x, training = True):
    """Computes NAM output by adding the outputs of individual feature nets."""
    individual_outputs = self.calc_outputs(x, training=training)
    stacked_out = tf.stack(individual_outputs, axis=-1)
    training = self._true if training else self._false
    dropout_out = tf.nn.dropout(
        stacked_out,
        rate=tf.cond(training, lambda: self._feature_dropout, lambda: 0.0))
    out = tf.reduce_sum(dropout_out, axis=-1)
    return out + self._bias

  def _name_scope(self):
    """Overrides the default function to fix name_scope for bias."""
    tf_name_scope = self._kwargs.get('name_scope', None)
    name_scope = super(NAM, self)._name_scope()
    if tf_name_scope:
      return tf_name_scope + '/' + name_scope
    else:
      return name_scope

  def calc_outputs(self, x, training = True):
    """Returns the output computed by each feature net."""
    training = self._true if training else self._false
    list_x = tf.split(x, self._num_inputs, axis=-2)
    return [
        self.feature_rnns[i](x_i, training=training)
        for i, x_i in enumerate(list_x)
    ]
  
  def score(self, x, y):
        # Compute model performance metric
        y_pred = self.predict(x)
        return np.mean(y_pred == y)  # Example metric: mean accuracy

tf.compat.v1.reset_default_graph()
namrnn_model = create_nam_model(
    x_train=X_train,
    dropout=0.0,
    num_basis_functions=64,
    activation='relu',
    trainable=True,
    shallow=False,
    name_scope='model')

_ = namrnn_model(X_train[:1])
namrnn_model.summary()

namrnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), 
                    loss= tf.keras.losses.MeanSquaredError(),)

namrnn_model.fit(X_train, y_train, epochs = 100, batch_size = 32)

namrnn_model.evaluate(X_test,y_test)

predicted_stock_price = namrnn_model.predict(X_test)
y_pred = predicted_stock_price
y_test = np.reshape(y_test, (759,1))
predicted_stock_price = np.reshape(predicted_stock_price, (759,1))
print("y_test.shape=",y_test.shape,
      "y_pred.shape=",predicted_stock_price.shape)

"""Uncomment to code below only if you want to plot the real and predicted values. Otherwise you will see the normalized real and normalized predicted values"""

# # Inverse scaling the training and test set
# training_set_original = sc.inverse_transform(training_set_scaled)
# test_set_original = sc.inverse_transform(test_set_scaled)

# # Extracting the independent and dependent variables for the training set
# # X_train contains all columns except the last column
# X_train = training_set_original[:,:-1]
# # y_train contains only the last column
# y_train = training_set_original[:,-1]

# # Extracting the independent and dependent variables for the test set
# # X_test contains all columns except the last column
# X_test = test_set_original[:,:-1]
# # y_test contains only the last column
# y_test = test_set_original[:,-1]

# # Inverse scaling the predicted stock price
# predicted_stock_price = y_pred*sc.scale_[-1]+sc.mean_[-1]

from sklearn import metrics as sk_metrics

def rmse(y_true, y_pred):
  """Root mean squared error between true and predicted values."""
  return float(np.sqrt(sk_metrics.mean_squared_error(y_true, y_pred)))

def mae(y_true, y_pred):
  """Mean Absolute Error between true and predicted values."""
  return float(sk_metrics.mean_absolute_error(y_true, y_pred)) 
def mse(y_true, y_pred):
  """Mean squared error between true and predicted values."""
  return float(sk_metrics.mean_squared_error(y_true, y_pred))

def r2_score(y_true, y_pred):
  """R-squared score between true and predicted values."""
  return float(sk_metrics.r2_score(y_true, y_pred)) 
  
def sigmoid(x):
  """Sigmoid function."""
  if isinstance(x, list):
    x = np.array(x)
  return np.where(x >= 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))

def calculate_metric(y_true,
                     predictions,
                     regression = True):
  """Calculates the evaluation metric."""
  if regression:
    return rmse(y_true, predictions)
  else:
    return sk_metrics.roc_auc_score(y_true, sigmoid(predictions))

test_metric = calculate_metric(
    y_test, predicted_stock_price, regression=True)
print(test_metric)

import matplotlib.pyplot as plt

# Set the size of the plot figure to 10x5 inches
plt.figure(figsize=(10, 5))
# Show a grid on the plot
plt.grid(visible=True, linestyle='-.') 
# Plot the actual stock prices in red
plt.plot(y_test, color = 'red', label = 'Real Stock Price')
# Plot the predicted stock prices in blue
plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted Stock Price')
# Add a title to the plot
plt.title('NAM-RNN Prediction', fontsize=20)
# Add a label to the x-axis
plt.xlabel('Time', fontsize=15)
# Add a label to the y-axis
plt.ylabel('Stock Price', fontsize=15)
# Add a legend to the plot
plt.legend(fontsize=15)
# Set the limits of the y-axis to ensure all actual and predicted stock prices are visible
plt.ylim(bottom=min(y_test) - (max(y_test) - min(y_test))*0.1, top=max(y_test) + (max(y_test) - min(y_test))*0.1)

# Save the plot as a PNG file
# plt.savefig("NAM-RNN.png")

plt.show()
# from google.colab import files
# plt.savefig('nam-rnn.png')
# files.download('nam-rnn.png')

import numpy as np
from sklearn.metrics import r2_score

y_pred = predicted_stock_price

# Calculate the baseline performance
baseline_r2 = r2_score(y_test, y_pred)

# Initialize a list to store the feature importance scores
permutation_scores = []

# Iterate over each feature
for feature in range(X_train.shape[1]):
    # Shuffle the values of the current feature
    X_train_shuffled = X_train.copy()
    X_train_shuffled[:, feature] = np.random.permutation(X_train[:, feature])

    # Re-fit the model and make predictions
    namrnn_model.fit(X_train_shuffled, y_train)
    y_pred_shuffled = namrnn_model.predict(X_test)

    # Calculate the decrease in performance
    r2 = r2_score(y_test, y_pred_shuffled)
    permutation_score = baseline_r2 - r2
    permutation_scores.append(permutation_score)

# normalize the feature importance to sum to 1
permutation_scores = np.array(permutation_scores)
permutation_scores /= permutation_scores.sum()

# Create a bar plot with x-axis values ranging from 0 to the number of columns in the X_train array
# and y-axis values set to permutation_scores, using blue color for bars and dark blue for edges.
plt.bar(range(X_train.shape[1]), permutation_scores, color='#00BFFF', edgecolor='#00008B')

# Set the x-ticks for the plot to be the names of the columns in the original data, rotated 45 degrees,
# and with a font size of 12.
plt.xticks(range(X_train.shape[1]), data.columns, rotation=45, fontsize=12)

# Set the title for the plot to "Feature Importance NAM-RNN", with a font size of 20.
plt.title('Feature Importance NAM-RNN', fontsize=20)

# Set the label for the y-axis to "Feature Importance Score", with a font size of 14.
plt.ylabel('Feature Importance Score', fontsize=14)

# Set the y-axis limits to be between 0 and the maximum value of permutation_scores plus 0.1.
plt.ylim(0, max(permutation_scores)+0.1)

# Display a grid with dashed lines in a light gray color.
plt.grid(visible=True, linestyle='--', color='#D3D3D3')

# Save the plot as an image file named "feature_importance NAM-RNN.png",
# with the plot boundaries tightly cropped.
# plt.savefig('feature_importance NAM-RNN.png', bbox_inches='tight')

plt.show()